/**
 * Script para testar a integra√ß√£o com modelos locais
 * Este script usa o adaptador LLM para se conectar a um servidor local e testar o processamento de mensagens
 * 
 * Instru√ß√µes:
 * 1. Instale e inicie o Ollama (https://ollama.ai/) ou outro servidor local compat√≠vel com a API OpenAI
 * 2. Configure a vari√°vel LOCAL_LLM_URL no .env apontando para o servidor
 * 3. Execute este script com: npx tsx server/test-local-llm.ts
 */

// importando o LLMAdapter
import { LLMAdapter } from './llm-adapter';

async function testLocalLLM() {
  console.log("üß™ Testando integra√ß√£o com LLM local");
  
  // URL do servidor local
  const baseUrl = process.env.LOCAL_LLM_URL || "http://localhost:11434/v1";
  const modelName = process.env.LOCAL_LLM_MODEL || 'Qwen3-32B-Q3_K_L.gguf';
  
  console.log(`‚ÑπÔ∏è Configura√ß√£o:`);
  console.log(`   URL do servidor: ${baseUrl}`);
  console.log(`   Modelo: ${modelName}`);
  
  try {
    // Tenta primeiro verificar se o servidor est√° online
    console.log(`\nüîç Verificando conex√£o com o servidor LLM...`);
    
    try {
      // Tentamos fazer uma requisi√ß√£o simples para verificar se o servidor est√° dispon√≠vel
      const response = await fetch(`${baseUrl}/models`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' }
      });
      
      if (response.ok) {
        const models = await response.json();
        console.log(`‚úÖ Servidor LLM est√° online!`);
        console.log(`üìã Modelos dispon√≠veis:`);
        
        if (Array.isArray(models.data)) {
          models.data.forEach((model: any) => {
            console.log(`   ‚Ä¢ ${model.id || model.name}`);
          });
        } else {
          console.log(`   Resposta do servidor: `, JSON.stringify(models).substring(0, 200) + "...");
        }
      } else {
        console.log(`‚ö†Ô∏è Servidor est√° online, mas retornou status ${response.status}`);
      }
    } catch (checkError: any) {
      throw new Error(`N√£o foi poss√≠vel conectar ao servidor: ${checkError.message}`);
    }
    
    // Cria um adaptador LLM configurado para usar o servidor local
    console.log(`\n‚öôÔ∏è Inicializando adaptador LLM para modelo ${modelName}...`);
    const llm = new LLMAdapter({
      provider: 'local',
      baseUrl: baseUrl,
      model: modelName
    });
    
    // Mensagem de teste
    const systemPrompt = "Voc√™ √© o assistente TarefoAI, especializado em ajudar com produtividade. Responda em portugu√™s do Brasil.";
    const userMessage = "Ol√°, pode me ajudar a organizar minha agenda para a pr√≥xima semana?";
    
    console.log("\nüìù Mensagem de teste:");
    console.log(`   Sistema: ${systemPrompt}`);
    console.log(`   Usu√°rio: ${userMessage}`);
    
    console.log("\nüîÑ Enviando solicita√ß√£o para o modelo local...");
    
    // Inicia o cron√¥metro para medir o tempo de resposta
    const startTime = Date.now();
    
    // Tenta obter uma resposta com timeout
    let timeoutId: NodeJS.Timeout;
    
    // Crie uma promise com timeout
    const responsePromise = Promise.race([
      llm.generateResponse([
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userMessage }
      ]),
      new Promise<string>((_, reject) => {
        timeoutId = setTimeout(() => {
          reject(new Error("Tempo limite excedido ao esperar resposta do servidor local (15s)"));
        }, 15000); // 15 segundos de timeout
      })
    ]);
    
    // Processa a mensagem
    const response = await responsePromise;
    clearTimeout(timeoutId!);
    
    // Calcula o tempo de resposta
    const responseTime = Date.now() - startTime;
    
    console.log("\n‚úÖ Resposta recebida do modelo local:");
    console.log("---------------------------------------");
    console.log(response);
    console.log("---------------------------------------");
    console.log(`‚è±Ô∏è Tempo de resposta: ${responseTime}ms (${(responseTime/1000).toFixed(2)}s)`);
    
    console.log("\n‚úÖ Teste com LLM local conclu√≠do com sucesso");
    
    console.log("\nüöÄ Para usar este modelo em produ√ß√£o:");
    console.log("   1. Configure as vari√°veis de ambiente:");
    console.log(`      LOCAL_LLM_URL=${baseUrl}`);
    console.log(`      LOCAL_LLM_MODEL=${modelName}`);
    console.log("   2. Reinicie o servidor");
    console.log("   3. O adaptador LLM usar√° automaticamente o modelo local");
    
    return true;
  } catch (error: any) {
    console.error("\n‚ùå Erro ao testar modelo local:");
    
    if (error?.message) {
      console.error(`   Mensagem: ${error.message}`);
    }
    
    if (error?.cause) {
      console.error(`   Causa: ${error.cause.message || JSON.stringify(error.cause)}`);
    }
    
    const isConnRefused = typeof error?.message === 'string' && 
      (error.message.includes('ECONNREFUSED') || 
       error.message.includes('Failed to fetch') || 
       error.message.includes('Tempo limite excedido') ||
       error.message.includes('N√£o foi poss√≠vel conectar'));
      
    if (isConnRefused) {
      console.log("\n‚ö†Ô∏è N√£o foi poss√≠vel conectar ao servidor LLM local:");
      console.log("   1. Verifique se o Ollama est√° instalado e em execu√ß√£o");
      console.log("   2. Verifique se o URL est√° correto (padr√£o: http://localhost:11434/v1)");
      console.log("   3. Certifique-se que o modelo est√° dispon√≠vel no servidor");
      
      console.log("\nüìã Instru√ß√µes para instalar Ollama com Qwen3:");
      console.log("   ‚Ä¢ Download Ollama: https://ollama.ai/download");
      console.log("   ‚Ä¢ Para baixar o modelo Qwen3: ollama pull qwen");
      console.log("   ‚Ä¢ OU para baixar o modelo quantizado: ollama pull qwen:7b-q4_0");
      console.log("   ‚Ä¢ Para usar Qwen3-32B-Q3_K_L.gguf, siga estas etapas:");
      console.log("     1. Baixe o arquivo GGUF do modelo");
      console.log("     2. Crie um arquivo Modelfile com:");
      console.log("        FROM Qwen3-32B-Q3_K_L.gguf");
      console.log("        PARAMETER temperature 0.7");
      console.log("     3. Execute: ollama create qwen3-32b -f Modelfile");
      console.log("     4. Inicie com: ollama run qwen3-32b");
      
      console.log("\nüîç Alternativas para testar em ambiente Replit:");
      console.log("   1. Configure um t√∫nel SSH (ex: ngrok) para seu Ollama local:");
      console.log("      ngrok http 11434");
      console.log("   2. Use o URL fornecido pelo ngrok em LOCAL_LLM_URL");
      console.log("   3. Ou use servi√ßos online com API compat√≠vel com OpenAI");
    }
    
    return false;
  }
}

// Executa o teste
testLocalLLM();